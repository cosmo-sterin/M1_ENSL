{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexte\n",
    "## Régression Linéaire, Erreur de Généralisation\n",
    "Soit $f: E \\subset [0,1]^d \\to [0,1]$ une fonction inconnue avec E **fini**.   \n",
    "On suppose que l'on connaît $f$ en quelques points :     \n",
    "On dispose de $D = \\{(x_i,y_i) \\, | \\, y_i = f(x_i)\\}$ fini avec $m = |D| << |E|$.   \n",
    "Le but de la **régression linéaire** est d'approcher $f$ au mieux avec $f_\\theta$, modèle linéaire, défini par :   \n",
    "\n",
    "$$ f_\\theta(x) = \\theta^{T}x$$\n",
    "Avec $\\theta \\in \\mathbb{R^d}$ le paramètre de $f_\\theta$.    \n",
    "Plus formellement on veut trouver $\\theta$ qui minimise **l'erreur absolue** :   \n",
    "$$ J(\\theta) = \\sum_{x \\in E} (f(x)-f_\\theta(x))^2 $$    \n",
    "Cependant comme on ne connaît pas $E$, on doit se contenter d'approcher $J$ par $\\hat J$ **l'erreur empirique** sur $D$ :\n",
    "$$ \\hat{J}(\\theta) =  \\sum_{i} (y_i-f_\\theta(x_i))^2 = \\sum_{i} (y_i-\\theta^{T}x_i)^2 $$ \n",
    "1) Que signifie $J(\\theta) = 0$ ? $\\hat{J}(\\theta) = 0$ ?   \n",
    "2) Montrer $$ \\hat{J}(\\theta) \\leq J(\\theta)$$     \n",
    "3) Minimiser $\\hat{J}(\\theta)$ revient-il à minimiser $J(\\theta)$ ?   \n",
    "Dans l'idéal on voudrait donc minimiser $J(\\theta)$, cependant c'est $\\hat{J}(\\theta)$ que l'on peut minimiser en pratique.   \n",
    "Ce qui nous intéresserait ce serait trouver un terme $M > 0$ tel que:    \n",
    "$$ J(\\theta^*) \\leq \\hat{J}(\\theta^*) + M$$   \n",
    "Avec $\\theta^*$ qui minimise $\\hat{J}$.    \n",
    "4) En quoi peut-on qualifier $M$ **d'erreur de généralisation** ?\n",
    "## Régression Linéaire Ridge-Régularisée\n",
    "Pour des raisons passées sous silence, on considère ici les minimisations de : \n",
    "$$ J_r(\\theta) = J(\\theta) + \\lambda ||\\theta||_2^2 $$\n",
    "$$\\hat{J}_r(\\theta) = \\hat{J}(\\theta) + \\lambda ||\\theta||_2^2$$   \n",
    "Avec $\\lambda$ un scalaire donné.   \n",
    "On parle alors de **régression linéaire ridge-régularisée**.  \n",
    "On admet qu'il existe un algorithme capable de fournir $\\theta^{*}$ qui minimise $\\hat{J}_r $.   \n",
    "On cherche toujours une inégalité du type :   \n",
    "$$ J_r(\\theta^{*}) \\leq \\hat{J}_r(\\theta^{*}) + M$$   \n",
    "# Stabilité Uniforme    \n",
    "Dans la suite on note:   \n",
    "-$\\theta$ qui minimise $\\hat{J}_r$ sur $D$   \n",
    "-$\\theta^{ \\backslash i}$  qui minimise $\\hat{J}_r$ sur $D \\backslash \\{(x_i,y_i)\\}$       \n",
    "     \n",
    "Pour trouver la borne $M$ on dispose du théorème suivant :   \n",
    "**Théorème : ** S'il existe $\\beta$ tel que :  \n",
    "$$ \\forall i \\in \\{1 \\dots m \\},  \\quad sup_{x \\in E} |l(\\theta,x)-l(\\theta^{ \\backslash i},x)| \\leq \\frac{\\beta}{m}$$   \n",
    "Avec $l(\\theta,x) = (\\theta^{T} x - f(x))^2$ et    $l(\\theta^{\\backslash i},x) = (\\theta^\\backslash{i}^{T} x - f(x))^2$    \n",
    "Et si $l(\\theta,x)$ **est bornée** par $M$  \n",
    "**Alors :**    \n",
    "Avec probabilité $1-\\delta$:     \n",
    "$$ J_r(\\theta) \\leq \\hat{J}_r(\\theta) + 2 \\frac{\\beta}{m} + (4\\beta + M)\\sqrt{\\frac{ln(\\frac{1}{\\delta})}{2m}} $$   \n",
    "5) Que se passe-t-il lorsque m est grand ? Comment l'interprétez vous ?\n",
    "# Trouvons $\\beta$\n",
    "Nous avons toujours    \n",
    "-$\\theta$ qui minimise $\\hat{J}_r$ sur $D$   \n",
    "-$\\theta^{ \\backslash i}$  qui minimise $\\hat{J}_r$ sur $D \\backslash \\{(x_i,y_i)\\}$        \n",
    "     \n",
    "6) Montrer que $||\\theta||^2_2 \\leq \\frac{1}{\\lambda}$    \n",
    "7) En déduire $l(\\theta,x) \\leq 1$      \n",
    "    \n",
    "On remplit donc la deuxième condition du théorème avec M=1.    \n",
    "Passons à la première.   \n",
    "    \n",
    "8) Montrer que $l$ est convexe en son premier argument   \n",
    "9) Montrer qu'il existe $\\sigma > 0$ tel que :\n",
    "$$ \\forall \\theta_1,\\theta_2 \\, \\forall x \\quad |l(\\theta_1,x)-l(\\theta_2,x)| \\leq \\sigma |(\\theta_1 - \\theta_2)^T x|$$      \n",
    "10) En déduire ce **lemme** :      \n",
    "$$ \\forall t \\in [0,1] \\quad ||\\theta||^2_2 - ||\\theta + t \\Delta \\theta||_2^2 + ||\\theta^{ \\backslash i}||_2^2 - ||\\theta^{ \\backslash i} - t \\Delta \\theta||_2^2 \\leq \\frac{t \\sigma}{\\lambda m}|\\Delta \\theta^T x_i| $$   \n",
    "Avec $\\Delta \\theta = \\theta^{ \\backslash i} - \\theta$.    \n",
    "11) Constater que pour $t=\\frac{1}{2}$ on a $||\\theta||^2_2 - ||\\theta + t \\Delta \\theta||_2^2 + ||\\theta^{ \\backslash i}||_2^2 - ||\\theta^{ \\backslash i} - t \\Delta \\theta||_2^2 = \\frac{1}{2}||\\Delta \\theta||^2_2$   \n",
    "12) En déduire une majoration de : \n",
    "$$ |l(\\theta,x)-l(\\theta^{ \\backslash i},x)| $$    \n",
    "13) Conclure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Correction\n",
    "Coming soon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources\n",
    "Sujet (adapté) : http://perso.univ-st-etienne.fr/habrarda/ENSML/td_uniform_stability.pdf     \n",
    "Correction : http://perso.univ-st-etienne.fr/habrarda/ENSML/recap_uniformStability.pdf        \n",
    "Papier d'origine : \n",
    "Olivier Bousquet, André Elisseeff: Stability and Generalization. Journal of Machine Learning Research 2: 499-526 (2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
